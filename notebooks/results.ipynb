{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import iglob\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"pastel\")\n",
    "mpl.rcParams['figure.figsize'] = (20, 20)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "#plt.rcParams[\"font.family\"] = \"Computer Modern\"\n",
    "plt.rcParams[\"font.size\"] = \"24\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://gist.github.com/willwhitney/9cecd56324183ef93c2424c9aa7a31b4\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "def load_tf(experiment):\n",
    "    globbing = f\"../experiments/*/{experiment}/tensorboard/*\"\n",
    "    found = list(iglob(globbing))\n",
    "    if len(found) < 1:\n",
    "        print(f\"no runs found in {globbing}, check your experiments folder\")\n",
    "        return None, None\n",
    "    print(f\"found {len(found)} experiments in {globbing}\")\n",
    "    pds = []\n",
    "    for dirname in found:\n",
    "        ea = event_accumulator.EventAccumulator(dirname, size_guidance={event_accumulator.SCALARS: 0})\n",
    "        ea.Reload()\n",
    "        dframes = {}\n",
    "        mnames = ea.Tags()['scalars']\n",
    "\n",
    "        base_name = dirname.split('/')[-1]\n",
    "        extras = {}\n",
    "        extras['experiment_folder'] = experiment\n",
    "        extras['experiment'] = base_name\n",
    "        extras['dataset'], rest = base_name.split('-', 1)\n",
    "        extras['loss_name'], rest = rest.split('-batch_', 1)\n",
    "        extras['batchsize'], rest = rest.split('-lr_', 1)\n",
    "        extras['batchsize'] = int(extras['batchsize'])\n",
    "        extras['lr'], rest = rest.split('_', 1)\n",
    "        extras['lr'] = float(extras['lr'])\n",
    "        \n",
    "        for n in mnames:\n",
    "            dframes[n] = pd.DataFrame(ea.Scalars(n), columns=[\"wall_time\", \"epoch\", n])\n",
    "            dframes[n].drop(\"wall_time\", axis=1, inplace=True)\n",
    "            dframes[n] = dframes[n].set_index(\"epoch\")\n",
    "        if len(dframes):\n",
    "            try:\n",
    "                pds.append(pd.concat([v for k,v in dframes.items()], axis=1))\n",
    "            except ValueError as e:\n",
    "                print(f\"Error loading: {dirname}\")\n",
    "                print(e)\n",
    "                continue\n",
    "            for k,v in extras.items():\n",
    "                pds[-1][k] = v\n",
    "    return pd.concat(pds), dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_NAMES = {\n",
    "    'accuracy_05': '$\\\\text{Accuracy}^l$',\n",
    "    'accuracy_05_sig_k10': '$\\\\text{Accuracy}^s$',\n",
    "    'f1_05': '$F_1^l$',\n",
    "    'f1_05_sig_k10': '$F_1^s$',\n",
    "    'bce': 'BCE'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    'main',\n",
    "    'auroc',\n",
    "    'wmw'\n",
    "]\n",
    "dfs = []\n",
    "for experiment in experiments:\n",
    "    df, path = load_tf(experiment)\n",
    "    if df is None:\n",
    "        continue\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find early stopping epoch/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for experiment in df['experiment'].unique():\n",
    "    _df = df[df['experiment']==experiment]\n",
    "    _df = _df.iloc[_df['val/loss'].idxmin()]\n",
    "    dfs.append(_df)\n",
    "best_by_val_loss = pd.concat(dfs, axis=1).T.reset_index().rename(columns={'index': 'epoch'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Count by Dataset + Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "filtered = []\n",
    "for dataset in best_by_val_loss['dataset'].unique():\n",
    "    for loss in best_by_val_loss['loss_name'].unique():\n",
    "        f = best_by_val_loss[(best_by_val_loss['loss_name']==loss)&(best_by_val_loss['dataset']==dataset)]\n",
    "        count = f.shape[0]\n",
    "        if count == 0:\n",
    "            print(\"no rows: \")\n",
    "            print(f\"  {count}x {dataset}, {loss}\")\n",
    "            continue\n",
    "        if count < n:\n",
    "            raise RuntimeError(f\"Cannot find at least 10 rows for condition: {count}x {dataset}, {loss}\")\n",
    "        filtered.append(f.head(n))\n",
    "        #print(f\"{count}x {dataset}, {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_by_val_loss_first_n = pd.concat(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "excluded_cols = set(['experiment'])\n",
    "dfs = []\n",
    "for loss in best_by_val_loss_first_n['loss_name'].unique():\n",
    "    for dataset in best_by_val_loss_first_n['dataset'].unique():\n",
    "        _df = best_by_val_loss_first_n[(best_by_val_loss_first_n['loss_name']==loss)&(best_by_val_loss_first_n['dataset']==dataset)]\n",
    "        _dfmean = _df.mean()\n",
    "        _dfstd = _df.std()\n",
    "        if np.isnan(_dfmean['loss']):\n",
    "            print(f\"{loss} {dataset} loss is NaN\")\n",
    "            continue\n",
    "        d = {'count': [_df.shape[0]]}\n",
    "        meancols = _dfmean.keys()\n",
    "        nonmeancols = set(_df.columns) - set(_dfmean.keys()) - excluded_cols\n",
    "        for col in nonmeancols:\n",
    "            d[col] = [_df[col].iloc[0]]\n",
    "        for col in meancols:\n",
    "            d[f\"{col}/mean\"] = [_dfmean[col]]\n",
    "            d[f\"{col}/std\"] = [_dfstd[col]]\n",
    "        dfs.append(pd.DataFrame(data=d))\n",
    "        \n",
    "aggregate = pd.concat(dfs)\n",
    "aggregate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.775px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
